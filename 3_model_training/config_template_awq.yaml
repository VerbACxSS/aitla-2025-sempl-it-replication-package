### model
model_name_or_path: [base_model]
template: qwen

### method
stage: sft
do_train: true
finetuning_type: lora

### booster
flash_attn: fa2
enable_liger_kernel: true

### dataset
dataset_dir: data
dataset: [train_dataset_name]
cutoff_len: 6144
max_samples: 1020
overwrite_cache: true
preprocessing_num_workers: 16

### train params
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
num_train_epochs: 5.0
warmup_ratio: 0.1
learning_rate: 2.0e-5
lr_scheduler_type: cosine
max_grad_norm: 1.0
packing: false
bf16: true
fp16: false

### lora
lora_target: q_proj,v_proj
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.05
loraplus_lr_ratio: 1
use_rslora: false
use_dora: false

### eval
# eval_dataset: [eval_dataset_name]
# per_device_eval_batch_size: 2
# eval_accumulation_steps: 8
# eval_strategy: steps
# eval_steps: 20
# predict_with_generate: true

### logging
logging_strategy: steps
logging_steps: 5
plot_loss: true
report_to: tensorboard

### output
output_dir: [output_dir]
save_strategy: epoch
overwrite_output_dir: true

## hub
hub_model_id: [hub_model_id]